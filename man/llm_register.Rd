% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/register_orchestrator.R
\name{llm_register}
\alias{llm_register}
\title{Register an LLM Endpoint with Pass-2 Validation}
\usage{
llm_register(
  url,
  provider = "official",
  headers,
  body,
  api_key,
  content_value = "Hello!",
  generation_interface = NULL,
  default_system_prompt = NULL,
  optional_defaults = NULL,
  role_mapping = NULL,
  timeout = 120,
  top_k = 5,
  ns_prob_thresh = list(answer = 0.6, think = 0.55),
  st_prob_thresh = list(answer = 0.7, think = 0.55),
  lexicon = default_keyword_lexicon(),
  stream_param = NULL,
  auto_register = FALSE
)
}
\arguments{
\item{url}{Character(1). Endpoint URL.}

\item{provider}{Character(1). Provider label stored in the registry. One of
\code{"official"}, \code{"vllm"}, \code{"proxy"}, \code{"custom"}.}

\item{headers}{Named list of HTTP headers. May include placeholders such as
\code{\${API_KEY}}; \code{Accept: text/event-stream} is added automatically
during probing if streaming requires it.}

\item{body}{List template for the request payload. May include placeholders
\code{\${ROLE}}, \code{\${CONTENT}}, and a parameter merge anchor
\code{\${PARAMETER}}. The raw Pass-1 body is preserved as a fallback in the
resulting registry entry.}

\item{api_key}{Character(1). API/Bearer key used to substitute
\code{\${API_KEY}} in headers/body during probes.}

\item{content_value}{Character(1). Test content for the user message during
probing (e.g., \code{"Hello!"}).}

\item{generation_interface}{Optional character(1). Interface name to record
in the entry (e.g., \code{"chat"}, \code{"messages"}, \code{"responses"}).
If \code{NULL}, it may be auto-classified from \code{url}.}

\item{default_system_prompt}{Optional character(1). Structural default system
message to include in the entry (if applicable). Not forced at runtime.}

\item{optional_defaults}{Named list of typed defaults (e.g.,
\code{list(stream = TRUE, max_tokens = 512)}). These are recorded in the
entry and used by callers unless explicitly overridden.}

\item{role_mapping}{Optional named list mapping roles (e.g.,
\code{list(system = "system", user = "user", assistant = "assistant")}).
If \code{NULL}, the function may infer mapping from \code{body} structure
for recording purposes; mapping is never *forced* unless a caller supplies it.}

\item{timeout}{Numeric(1). Per-probe timeout in seconds.}

\item{top_k}{Integer(1). Number of near-miss candidates to keep in the
printed ranking tables.}

\item{ns_prob_thresh}{Named list with probability thresholds for Pass-1/2
non-stream selection, e.g. \code{list(answer = 0.60, think = 0.55)}.}

\item{st_prob_thresh}{Named list with probability thresholds for Pass-1/2
streaming selection, e.g. \code{list(answer = 0.70, think = 0.55)}.}

\item{lexicon}{Keyword lexicon (list) for candidate scoring.}

\item{stream_param}{Character scalar (optional).
Name of the request-body field used by the provider to enable streaming.
For most OpenAI-compatible APIs this is `"stream"`, but some endpoints
use alternative keys such as `"streaming"`, `"enable_sse"`, or `"use_stream"`.
If `NULL`, the function falls back to `"stream".}

\item{auto_register}{Logical(1). If \code{TRUE}, the composed entry is
immediately written/merged into the user registry; otherwise only a preview
is printed.}
}
\value{
Invisibly returns a list with components:
\itemize{
  \item \code{report}: Character vector of human-readable log lines.
  \item \code{pass1}: List with raw/effective inputs, HTTP details, ranked
        candidates, and selected ports (\code{respond_path}, \code{thinking_path},
        \code{delta_path}, \code{thinking_delta_path}).
  \item \code{pass2}: List with HTTP details and any Pass-2 warnings.
}
}
\description{
Orchestrates a two-pass analysis to register an arbitrary LLM endpoint into
the PsyLingLLM registry. **Pass-1** probes the raw endpoint (non-stream and
streaming/SSE) and scores likely extraction paths for `answer` and `thinking`
fields. **Pass-2** builds a *standardized* request template (using the
provider's own field names), re-probes the endpoint, and verifies that the
selected ports are still retrievable. Finally, it composes a registry entry
and, if requested, writes/merges it into the user registry.
}
\details{
This function prints a human-readable report (status, candidate paths,
stream capability, usage fields, and Pass-1 vs Pass-2 consistency). It
returns a structured object for programmatic inspection.


**What “ports retrievability” means**: the Pass-2 check verifies that the
JSON paths chosen in Pass-1 (for answer/thinking and streaming deltas) can be
rediscovered from the standardized request. It does not guarantee semantic
equivalence of outputs, nor does it attempt to reconcile vendor-specific
streaming event semantics beyond path retrievability.

**Safety & side-effects**: this function makes live requests to the given
endpoint. Avoid running in examples/tests against production APIs unless
guarded with \code{\\dontrun{}} or environment checks.
}
\section{Workflow}{

1. **Pass-1 probe**: send non-streaming POST; attempt SSE streaming; parse and
   score candidate JSON paths for `answer`/`thinking` (and streaming deltas).
2. **Standardize**: construct Pass-2 headers/body that preserve vendor keys,
   with placeholders \code{\${ROLE}}, \code{\${CONTENT}}, and a parameter
   merge anchor \code{\${PARAMETER}}; apply typed defaults.
3. **Pass-2 probe**: re-issue the request; verify that the chosen ports are
   still discoverable (retrievability only; not a semantic equivalence test).
4. **Registry preview & upsert**: compose a registry entry (input/output/
   streaming blocks) and optionally write it into the user registry.
}

\seealso{
\code{\link{probe_llm_streaming}},
\code{\link{score_candidates_ns}},
\code{\link{score_candidates_st}},
\code{\link{build_standardized_input}},
\code{\link{make_pass2_probe_inputs}},
\code{\link{render_pass2_path_consistency_report}},
\code{\link{build_registry_entry_from_analysis}},
\code{\link{register_endpoint_to_user_registry}},
\code{\link{get_registry_entry}},
\code{\link{llm_caller}}

# Example: OpenAI-compatible chat endpoint
\code{
llm_register(
  url      = "https://api.openai.com/v1/chat/completions",
  provider = "official",
  headers  = list(
    "Content-Type" = "application/json",
    "Authorization" = "Bearer \\${API_KEY}"
  ),
  body     = list(
    model    = "gpt-4o-mini",
    messages = list(list(role = "user", content = "\\${CONTENT}")),
    stream   = TRUE
  ),
  api_key  = Sys.getenv("OPENAI_API_KEY"),
  content_value = "Hello from PsyLingLLM!",
  generation_interface = "chat",
  optional_defaults   = list(stream = TRUE, max_tokens = 512),
  auto_register = FALSE   # preview only
)
}

# Example: DeepSeek-style endpoint (adjust URL/model to your provider)
\code{
llm_register(
  url      = "https://api.deepseek.com/v1/chat/completions",
  provider = "proxy",
  headers  = list(
    "Content-Type" = "application/json",
    "Authorization" = "Bearer \\${API_KEY}"
  ),
  body     = list(
    model    = "deepseek-chat",
    messages = list(list(role = "user", content = "\\${CONTENT}")),
    stream   = TRUE
  ),
  api_key  = Sys.getenv("DEEPSEEK_API_KEY"),
  generation_interface = "chat",
  auto_register = TRUE
)
}
}
\keyword{llm}
\keyword{registry}
\keyword{sse}
\keyword{streaming}
