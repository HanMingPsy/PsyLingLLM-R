% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_caller.R
\name{llm_caller}
\alias{llm_caller}
\title{LLM API Caller with integrated prompt preparation}
\usage{
llm_caller(
  model,
  trial_prompt = NULL,
  material = NULL,
  conversation = NULL,
  api_key,
  api_url,
  custom = list(user = "user", system = "system", assistant = "assistant"),
  system_prompt = "You are a participant in a psychology experiment.",
  max_tokens = 1024,
  temperature = 0.7,
  enable_thinking = TRUE
)
}
\arguments{
\item{model}{Model name}

\item{trial_prompt}{Trial prompt text (used if conversation is NULL)}

\item{material}{Trial material text (used if conversation is NULL)}

\item{conversation}{Optional list of messages with roles and content for conversation history
e.g., list(list(role="user", content="Hello"), list(role="assistant", content="Hi"))}

\item{api_key}{API key}

\item{api_url}{API endpoint}

\item{custom}{Optional role mapping (default: user/system/assistant)}

\item{max_tokens}{Maximum number of tokens to generate}

\item{temperature}{Sampling temperature}

\item{enable_thinking}{Boolean, whether to enable CoT reasoning}
}
\value{
List with parsed output: model, think, response
}
\description{
Call a large language model (LLM) API and return parsed output,
automatically handling model-specific fast/thinking modes.
Supports optional conversation history.
}
